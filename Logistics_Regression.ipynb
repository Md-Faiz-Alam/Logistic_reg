{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "---\n",
        "#Logistics Regression Theoretical Answers:-\n",
        "---\n",
        "---\n",
        "###1.  What is Logistic Regression, and how does it differ from Linear Regression?\n",
        "Logistic Regression is a classification algorithm used to predict the probability of a binary outcome (0 or 1).\n",
        "Linear Regression is used for predicting continuous numerical values.\n",
        "\n",
        "Key difference:\n",
        "\n",
        "Linear Regression outputs continuous values.\n",
        "\n",
        "Logistic Regression outputs probabilities (between 0 and 1), which are then mapped to classes using a threshold (usually 0.5).\n",
        "\n",
        "---\n",
        "### 2. What is the mathematical equation of Logistic Regression?\n",
        "‚Ñé\n",
        "(\n",
        "ùë•\n",
        ")\n",
        "=\n",
        "1/(\n",
        "1\n",
        "+\n",
        "ùëí\n",
        "‚àí\n",
        "(\n",
        "ùõΩ\n",
        "0\n",
        "+\n",
        "ùõΩ\n",
        "1\n",
        "ùë•\n",
        "1\n",
        "+\n",
        "ùõΩ\n",
        "2\n",
        "ùë•\n",
        "2\n",
        "+\n",
        "‚ãØ\n",
        "+\n",
        "ùõΩ\n",
        "ùëõ\n",
        "ùë•\n",
        "ùëõ\n",
        "))\n",
        "\n",
        "Where:\n",
        "h(x) is the predicted probability,\n",
        "\n",
        "Œ≤s are model parameters (weights),\n",
        "\n",
        "xs are features,\n",
        "\n",
        "The expression inside the exponent is a linear combination of inputs.\n",
        "\n",
        "---\n",
        "### 3. Why do we use the Sigmoid function in Logistic Regression?\n",
        "The sigmoid function maps any real-valued number to a value between 0 and 1, making it perfect for binary classification, where we need probabilities.\n",
        "\n",
        "ùúé\n",
        "(\n",
        "ùëß\n",
        ")\n",
        "=\n",
        "1/(\n",
        "1\n",
        "+\n",
        "ùëí\n",
        "‚àí\n",
        "ùëß)\n",
        "\n",
        "---\n",
        "### 4. What is the cost function of Logistic Regression?\n",
        "Logistic Regression uses Log Loss (Binary Cross-Entropy) as its cost function:\n",
        "\n",
        "J(Œ∏)=‚àí\n",
        "m\n",
        "1\n",
        "‚Äã\n",
        "  \n",
        "i=1\n",
        "‚àë\n",
        "m\n",
        "‚Äã\n",
        " [y\n",
        "(i)\n",
        " log(h(x\n",
        "(i)\n",
        " ))+(1‚àíy\n",
        "(i)\n",
        " )log(1‚àíh(x\n",
        "(i)\n",
        " ))]\n",
        "\n",
        "---\n",
        "### 5. What is Regularization in Logistic Regression? Why is it needed?\n",
        "Regularization adds a penalty to the loss function to reduce overfitting by discouraging overly complex models. It helps improve generalization on unseen data.\n",
        "\n",
        "---\n",
        "### 6. Explain the difference between Lasso, Ridge, and Elastic Net regression.\n",
        "\n",
        "Difference Between Lasso, Ridge, and Elastic Net Regression\n",
        "\n",
        "| Regression Type | Regularization Term                        | Feature Selection | Handles Multicollinearity | Main Use Case                                |\n",
        "|------------------|--------------------------------------------|--------------------|-----------------------------|-----------------------------------------------|\n",
        "| **Ridge**        | \\( \\lambda \\sum_{j=1}^{n} \\beta_j^2 \\)     | ‚ùå No              | ‚úÖ Yes                      | When all features are important, avoids overfitting |\n",
        "| **Lasso**        | \\( \\lambda \\sum_{j=1}^{n} |\\beta_j| \\)     | ‚úÖ Yes             | ‚ùå No (can struggle with correlated features) | When we want to perform feature selection     |\n",
        "| **Elastic Net**  | \\( \\lambda_1 \\sum |\\beta_j| + \\lambda_2 \\sum \\beta_j^2 \\) | ‚úÖ Yes             | ‚úÖ Yes                      | When features are many and/or correlated       |\n",
        "\n",
        "---\n",
        "### 7. When should we use Elastic Net instead of Lasso or Ridge?\n",
        "Use Elastic Net when:\n",
        "\n",
        "There are many correlated features.\n",
        "\n",
        "You want both feature selection (Lasso) and weight shrinkage (Ridge). It combines the strengths of both.\n",
        "\n",
        "---\n",
        "### 8. What is the impact of the regularization parameter (Œª) in Logistic Regression?\n",
        "High Œª: More regularization ‚Üí simpler model ‚Üí may underfit.\n",
        "\n",
        "Low Œª: Less regularization ‚Üí model can fit training data better ‚Üí risk of overfitting.\n",
        "It controls the trade-off between fitting well and keeping the model simple.\n",
        "\n",
        "---\n",
        "### 9. What are the key assumptions of Logistic Regression?\n",
        "The dependent variable is binary.\n",
        "\n",
        "Observations are independent.\n",
        "\n",
        "No multicollinearity among predictors.\n",
        "\n",
        "A linear relationship exists between the independent variables and the log-odds of the outcome.\n",
        "\n",
        "Large sample size for stable results.\n",
        "\n",
        "---\n",
        "### 10. What are some alternatives to Logistic Regression for classification tasks?\n",
        "Decision Trees\n",
        "\n",
        "Random Forest\n",
        "\n",
        "Support Vector Machines (SVM)\n",
        "\n",
        "K-Nearest Neighbors (KNN)\n",
        "\n",
        "Naive Bayes\n",
        "\n",
        "Neural Networks\n",
        "\n",
        "Gradient Boosting (XGBoost, LightGBM)\n",
        "\n",
        "---\n",
        "### 11. What are Classification Evaluation Metrics?\n",
        "Used to assess model performance. Key metrics:\n",
        "\n",
        "Accuracy: Overall correctness.\n",
        "\n",
        "Precision: Correct positive predictions / total predicted positives.\n",
        "\n",
        "Recall (Sensitivity): Correct positive predictions / actual positives.\n",
        "\n",
        "F1 Score: Harmonic mean of precision and recall.\n",
        "\n",
        "ROC-AUC: Measures the area under the ROC curve (good for imbalanced classes).\n",
        "\n",
        "Confusion Matrix: Table showing TP, TN, FP, FN.\n",
        "\n",
        "---\n",
        "### 12. How does class imbalance affect Logistic Regression?\n",
        "Class imbalance causes the model to favor the majority class, leading to high accuracy but poor performance in predicting the minority class. This is especially problematic in critical areas like disease detection or fraud detection.\n",
        "\n",
        "Solutions:\n",
        "\n",
        "Use techniques like SMOTE, class weights, resampling, or evaluation metrics like F1-score and AUC instead of accuracy.\n",
        "\n",
        "---\n",
        "### 13. What is Hyperparameter Tuning in Logistic Regression?\n",
        "It‚Äôs the process of finding the best values for parameters like:\n",
        "\n",
        "Regularization strength (Œª or C)\n",
        "\n",
        "Penalty type (L1, L2)\n",
        "\n",
        "Solver (like liblinear, saga, etc.)\n",
        "\n",
        "Tools: GridSearchCV, RandomizedSearchCV\n",
        "\n",
        "---\n",
        "### 14. What are different solvers in Logistic Regression? Which one should be used?\n",
        "\n",
        "Solvers in Logistic Regression\n",
        "\n",
        "| Solver       | Supports            | Best For                            | Notes                                  |\n",
        "|--------------|---------------------|-------------------------------------|----------------------------------------|\n",
        "| **liblinear**| L1, L2              | Small datasets, binary classification | One-vs-Rest (OvR) by default           |\n",
        "| **saga**     | L1, L2, Elastic Net | Large datasets, sparse data         | Supports multiclass with OvR & Softmax |\n",
        "| **lbfgs**    | L2                  | Multiclass, large datasets          | Efficient and widely used              |\n",
        "| **newton-cg**| L2                  | Multiclass problems                 | Slower than lbfgs                      |\n",
        "| **sag**      | L2\n",
        "\n",
        "---\n",
        "### 15. How is Logistic Regression extended for multiclass classification?\n",
        "One-vs-Rest (OvR): One classifier per class vs all others.\n",
        "\n",
        "Softmax Regression (Multinomial Logistic Regression): One model predicts probabilities for all classes using the softmax function.\n",
        "\n",
        "---\n",
        "### 16. What are the advantages and disadvantages of Logistic Regression?\n",
        "Advantages:\n",
        "\n",
        "Easy to implement and interpret\n",
        "\n",
        "Works well for linearly separable data\n",
        "\n",
        "Probabilistic output\n",
        "\n",
        "Disadvantages:\n",
        "\n",
        "Poor performance with non-linear relationships\n",
        "\n",
        "Sensitive to outliers and irrelevant features\n",
        "\n",
        "Assumes linear decision boundary in log-odds\n",
        "\n",
        "---\n",
        "### 17. What are some use cases of Logistic Regression?\n",
        "Disease diagnosis (e.g., cancer detection)\n",
        "\n",
        "Credit scoring and fraud detection\n",
        "\n",
        "Email spam detection\n",
        "\n",
        "Customer churn prediction\n",
        "\n",
        "Binary sentiment classification\n",
        "\n",
        "---\n",
        "### 18. What is the difference between Softmax Regression and Logistic Regression?\n",
        "\n",
        "Logistic Regression vs Softmax Regression\n",
        "\n",
        "| Feature                    | Logistic Regression         | Softmax Regression (Multinomial)   |\n",
        "|---------------------------|-----------------------------|------------------------------------|\n",
        "| Output                    | Probability of 1 class (binary) | Probabilities of all classes       |\n",
        "| Number of classes         | 2                           | More than 2                        |\n",
        "| Activation Function       | Sigmoid                     | Softmax                            |\n",
        "| Use Case                  | Binary classification       | Multiclass classification          |\n",
        "| Model Type in scikit-learn| `binary` (default)          | `multinomial`                      |\n",
        "| Common Solver             | liblinear                   | lbfgs, saga                        |\n",
        "\n",
        "\n",
        "---\n",
        "### 19. How do we choose between One-vs-Rest (OvR) and Softmax for multiclass classification?\n",
        "OvR: Simple, efficient when classes are imbalanced or independent.\n",
        "\n",
        "Softmax (Multinomial): Better when classes are mutually exclusive and interdependent.\n",
        "Use multinomial with solver like lbfgs or saga for Softmax in scikit-learn.\n",
        "\n",
        "---\n",
        "### 20. How do we interpret coefficients in Logistic Regression?\n",
        "\n",
        "Each coefficient\n",
        "ùõΩ\n",
        "ùëó\n",
        "Œ≤\n",
        "j\n",
        "‚Äã\n",
        "  represents the log-odds change of the outcome for a one-unit increase in the predictor\n",
        "ùë•\n",
        "ùëó\n",
        "x\n",
        "j\n",
        "‚Äã\n",
        " , holding others constant.\n",
        "\n",
        "To interpret in terms of odds:\n",
        "\n",
        "Odds¬†Ratio\n",
        "=\n",
        "ùëí\n",
        "ùõΩ\n",
        "ùëó\n",
        "Odds¬†Ratio=e\n",
        "Œ≤\n",
        "j\n",
        "‚Äã\n",
        "\n",
        "\n",
        "If\n",
        "ùõΩ\n",
        "ùëó\n",
        ">\n",
        "0\n",
        "Œ≤\n",
        "j\n",
        "‚Äã\n",
        " >0: Feature increases the probability of class 1.\n",
        "\n",
        "If\n",
        "ùõΩ\n",
        "ùëó\n",
        "<\n",
        "0\n",
        "Œ≤\n",
        "j\n",
        "‚Äã\n",
        " <0: Feature decreases it.\n",
        "\n",
        "---\n",
        "---\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "qPrl3I6zbtuG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Practical Answers:-\n",
        "\n",
        "### 1. Write a Python program that loads a dataset, splits it into training and testing sets, applies Logistic Regression, and prints the model accuracy."
      ],
      "metadata": {
        "id": "l8YRqHms607_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.datasets import load_breast_cancer\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load dataset\n",
        "data = load_breast_cancer()\n",
        "X, y = data.data, data.target\n",
        "\n",
        "# Split\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Train\n",
        "model = LogisticRegression(max_iter=1000)\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Predict and evaluate\n",
        "y_pred = model.predict(X_test)\n",
        "print(\"Accuracy:\", accuracy_score(y_test, y_pred))"
      ],
      "metadata": {
        "id": "784I3LAA7dzV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2. Write a Python program to apply L1 regularization (Lasso) on a dataset using LogisticRegression(penalty='l1') and print the model accuracy."
      ],
      "metadata": {
        "id": "GP_GZ1aC7sYc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model_l1 = LogisticRegression(penalty='l1', solver='liblinear', max_iter=1000)\n",
        "model_l1.fit(X_train, y_train)\n",
        "y_pred_l1 = model_l1.predict(X_test)\n",
        "print(\"L1 Regularization Accuracy:\", accuracy_score(y_test, y_pred_l1))"
      ],
      "metadata": {
        "id": "tfTS9BUK70Js"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 3. Write a Python program to train Logistic Regression with L2 regularization (Ridge) using LogisticRegression(penalty='l2'). Print model accuracy and coefficients."
      ],
      "metadata": {
        "id": "KmeY3MNu74Ys"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model_l2 = LogisticRegression(penalty='l2', solver='liblinear', max_iter=1000)\n",
        "model_l2.fit(X_train, y_train)\n",
        "y_pred_l2 = model_l2.predict(X_test)\n",
        "\n",
        "print(\"L2 Regularization Accuracy:\", accuracy_score(y_test, y_pred_l2))\n",
        "print(\"Model Coefficients:\", model_l2.coef_)"
      ],
      "metadata": {
        "id": "NBfriLvT8A40"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 4. Write a Python program to train Logistic Regression with Elastic Net Regularization (penalty='elasticnet')."
      ],
      "metadata": {
        "id": "BRoR4BRO8DyU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model_elastic = LogisticRegression(penalty='elasticnet', solver='saga', l1_ratio=0.5, max_iter=1000)\n",
        "model_elastic.fit(X_train, y_train)\n",
        "y_pred_elastic = model_elastic.predict(X_test)\n",
        "print(\"Elastic Net Accuracy:\", accuracy_score(y_test, y_pred_elastic))"
      ],
      "metadata": {
        "id": "uZzkgPyu8Ixj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 5. Write a Python program to train a Logistic Regression model for multiclass classification using multi_class='ovr'."
      ],
      "metadata": {
        "id": "IcHqF9LI8QsT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.datasets import load_iris\n",
        "\n",
        "iris = load_iris()\n",
        "X_iris, y_iris = iris.data, iris.target\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X_iris, y_iris, test_size=0.2, random_state=42)\n",
        "\n",
        "model_ovr = LogisticRegression(multi_class='ovr', solver='liblinear', max_iter=1000)\n",
        "model_ovr.fit(X_train, y_train)\n",
        "y_pred_ovr = model_ovr.predict(X_test)\n",
        "\n",
        "print(\"OvR Multiclass Accuracy:\", accuracy_score(y_test, y_pred_ovr))"
      ],
      "metadata": {
        "id": "wm90BbdJ8Wir"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 6. Write a Python program to apply GridSearchCV to tune the hyperparameters (C and penalty) of Logistic Regression. Print the best parameters and accuracy."
      ],
      "metadata": {
        "id": "i7Y49-D58auL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import GridSearchCV\n",
        "\n",
        "param_grid = {\n",
        "    'C': [0.01, 0.1, 1, 10],\n",
        "    'penalty': ['l1', 'l2'],\n",
        "    'solver': ['liblinear']\n",
        "}\n",
        "\n",
        "grid = GridSearchCV(LogisticRegression(max_iter=1000), param_grid, cv=5)\n",
        "grid.fit(X, y)\n",
        "\n",
        "print(\"Best Parameters:\", grid.best_params_)\n",
        "print(\"Best Accuracy:\", grid.best_score_)"
      ],
      "metadata": {
        "id": "-MQT9fxg8gPr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 7. Write a Python program to evaluate Logistic Regression using Stratified K-Fold Cross-Validation. Print the average accuracy."
      ],
      "metadata": {
        "id": "Ei1tU2pC8j7K"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import StratifiedKFold, cross_val_score\n",
        "\n",
        "skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
        "model_cv = LogisticRegression(max_iter=1000)\n",
        "\n",
        "scores = cross_val_score(model_cv, X, y, cv=skf)\n",
        "print(\"Cross-Validation Scores:\", scores)\n",
        "print(\"Average Accuracy:\", scores.mean())"
      ],
      "metadata": {
        "id": "rdNPv9mW8psy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 8. Write a Python program to load a dataset from a CSV file, apply Logistic Regression, and evaluate its accuracy."
      ],
      "metadata": {
        "id": "yvLgmiNG8sxC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "df = pd.read_csv('your_dataset.csv')\n",
        "# Assuming the last column is the target\n",
        "X = df.iloc[:, :-1]\n",
        "y = df.iloc[:, -1]\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "model_csv = LogisticRegression(max_iter=1000)\n",
        "model_csv.fit(X_train, y_train)\n",
        "\n",
        "y_pred_csv = model_csv.predict(X_test)\n",
        "print(\"Accuracy on CSV data:\", accuracy_score(y_test, y_pred_csv))\n"
      ],
      "metadata": {
        "id": "4rXBe5Gp8yq6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 9. Write a Python program to apply RandomizedSearchCV for tuning hyperparameters (C, penalty, solver) in Logistic Regression. Print the best parameters and accuracy."
      ],
      "metadata": {
        "id": "cl04Ddpj88HJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import RandomizedSearchCV\n",
        "from scipy.stats import uniform\n",
        "\n",
        "# Define parameter distribution\n",
        "param_dist = {\n",
        "    'C': uniform(0.1, 10),\n",
        "    'penalty': ['l1', 'l2', 'elasticnet'],\n",
        "    'solver': ['saga'],\n",
        "    'l1_ratio': [0.0, 0.25, 0.5, 0.75, 1.0]  # Only used if penalty is 'elasticnet'\n",
        "}\n",
        "\n",
        "model = LogisticRegression(max_iter=1000)\n",
        "random_search = RandomizedSearchCV(model, param_distributions=param_dist, n_iter=10, cv=5, scoring='accuracy', random_state=42)\n",
        "random_search.fit(X_train, y_train)\n",
        "\n",
        "print(\"Best Parameters:\", random_search.best_params_)\n",
        "print(\"Best Accuracy:\", random_search.best_score_)"
      ],
      "metadata": {
        "id": "YoF6Zcfhfrvc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###10. Write a Python program to implement One-vs-One (OvO) Multiclass Logistic Regression and print accuracy."
      ],
      "metadata": {
        "id": "0u3IcjP6f1Lz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.multiclass import OneVsOneClassifier\n",
        "\n",
        "# Load Iris dataset for multiclass classification\n",
        "from sklearn.datasets import load_iris\n",
        "iris = load_iris()\n",
        "X, y = iris.data, iris.target\n",
        "\n",
        "# Train-test split\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Logistic Regression with One-vs-One\n",
        "ovo_model = OneVsOneClassifier(LogisticRegression(max_iter=1000))\n",
        "ovo_model.fit(X_train, y_train)\n",
        "accuracy = ovo_model.score(X_test, y_test)\n",
        "\n",
        "print(\"Program 10 - One-vs-One Accuracy:\", accuracy)"
      ],
      "metadata": {
        "id": "1BQ2KbH1f6Ni"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###11. Write a Python program to train a Logistic Regression model and visualize the confusion matrix for binary classification."
      ],
      "metadata": {
        "id": "QUhk2YOFf93b"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.metrics import confusion_matrix\n",
        "\n",
        "# Train model\n",
        "model = LogisticRegression(max_iter=1000)\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Predict\n",
        "y_pred = model.predict(X_test)\n",
        "\n",
        "# Confusion matrix\n",
        "cm = confusion_matrix(y_test, y_pred)\n",
        "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues')\n",
        "plt.title('Confusion Matrix')\n",
        "plt.xlabel('Predicted')\n",
        "plt.ylabel('Actual')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "-sp6YlTigDgK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 12. Write a Python program to train a Logistic Regression model and evaluate its performance using Precision, Recall, and F1-Score."
      ],
      "metadata": {
        "id": "-g8D7ijZgmiZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import precision_score, recall_score, f1_score\n",
        "\n",
        "# Train model\n",
        "model = LogisticRegression(max_iter=1000)\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Predict\n",
        "y_pred = model.predict(X_test)\n",
        "\n",
        "# Evaluation\n",
        "precision = precision_score(y_test, y_pred)\n",
        "recall = recall_score(y_test, y_pred)\n",
        "f1 = f1_score(y_test, y_pred)\n",
        "\n",
        "print(\"Precision:\", precision)\n",
        "print(\"Recall:\", recall)\n",
        "print(\"F1-Score:\", f1)"
      ],
      "metadata": {
        "id": "wbAwShgwgsQ5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###13. Write a Python program to train a Logistic Regression model on imbalanced data and apply class weights to improve model performance."
      ],
      "metadata": {
        "id": "jT791DFJgzrB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Assume the original dataset is already imbalanced (e.g., breast cancer dataset)\n",
        "# Apply class weights to handle imbalance\n",
        "\n",
        "model = LogisticRegression(class_weight='balanced', max_iter=1000)\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Predict and evaluate\n",
        "y_pred = model.predict(X_test)\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "\n",
        "print(\"Accuracy with Class Weights:\", accuracy)"
      ],
      "metadata": {
        "id": "iDwjVKIog5VA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###14. Write a Python program to train Logistic Regression on the Titanic dataset, handle missing values, and evaluate performance."
      ],
      "metadata": {
        "id": "18eA8_FNg_jB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn.impute import SimpleImputer\n",
        "\n",
        "# Load Titanic dataset\n",
        "titanic = pd.read_csv('titanic.csv')\n",
        "\n",
        "# Handle missing values (e.g., using median for age)\n",
        "imputer = SimpleImputer(strategy='median')\n",
        "titanic['Age'] = imputer.fit_transform(titanic[['Age']])\n",
        "\n",
        "# Select features and target variable\n",
        "X = titanic[['Pclass', 'Age', 'SibSp', 'Parch', 'Fare']]\n",
        "y = titanic['Survived']\n",
        "\n",
        "# Train-test split\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Train Logistic Regression model\n",
        "model = LogisticRegression(max_iter=1000)\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Predict and evaluate\n",
        "y_pred = model.predict(X_test)\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "\n",
        "print(\"Titanic Logistic Regression Accuracy:\", accuracy)"
      ],
      "metadata": {
        "id": "Y61RJfEQhE2Q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###15. Write a Python program to apply feature scaling (Standardization) before training a Logistic Regression model. Evaluate its accuracy and compare results with and without scaling."
      ],
      "metadata": {
        "id": "yRpWk1KrhPMI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Standardize features\n",
        "scaler = StandardScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "X_test_scaled = scaler.transform(X_test)\n",
        "\n",
        "# Train Logistic Regression model without scaling\n",
        "model_no_scale = LogisticRegression(max_iter=1000)\n",
        "model_no_scale.fit(X_train, y_train)\n",
        "y_pred_no_scale = model_no_scale.predict(X_test)\n",
        "accuracy_no_scale = accuracy_score(y_test, y_pred_no_scale)\n",
        "\n",
        "# Train Logistic Regression model with scaling\n",
        "model_scaled = LogisticRegression(max_iter=1000)\n",
        "model_scaled.fit(X_train_scaled, y_train)\n",
        "y_pred_scaled = model_scaled.predict(X_test_scaled)\n",
        "accuracy_scaled = accuracy_score(y_test, y_pred_scaled)\n",
        "\n",
        "# Compare accuracy\n",
        "print(\"Accuracy without Scaling:\", accuracy_no_scale)\n",
        "print(\"Accuracy with Scaling:\", accuracy_scaled)"
      ],
      "metadata": {
        "id": "ZyMfZ3pZhcDH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###16. Write a Python program to train Logistic Regression and evaluate its performance using ROC-AUC score."
      ],
      "metadata": {
        "id": "3dZGPM0zhdy3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import roc_auc_score\n",
        "\n",
        "# Train Logistic Regression model\n",
        "model = LogisticRegression(max_iter=1000)\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Predict probabilities\n",
        "y_prob = model.predict_proba(X_test)[:, 1]\n",
        "\n",
        "# ROC-AUC score\n",
        "roc_auc = roc_auc_score(y_test, y_prob)\n",
        "print(\"ROC-AUC Score:\", roc_auc)"
      ],
      "metadata": {
        "id": "OG_kSFPDhh1f"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###17. Write a Python program to train Logistic Regression using a custom learning rate (C=0.5) and evaluate accuracy."
      ],
      "metadata": {
        "id": "9ODJ79I8hm3P"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Train Logistic Regression model with custom regularization strength (C)\n",
        "model = LogisticRegression(C=0.5, max_iter=1000)\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Predict and evaluate\n",
        "y_pred = model.predict(X_test)\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "\n",
        "print(\"Accuracy with custom learning rate (C=0.5):\", accuracy)"
      ],
      "metadata": {
        "id": "K0-v9A1LhrpX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###18. Write a Python program to train Logistic Regression and identify important features based on model coefficients."
      ],
      "metadata": {
        "id": "Xx2e6CWPhvf3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "# Train Logistic Regression model\n",
        "model = LogisticRegression(max_iter=1000)\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Get the coefficients of the model\n",
        "coefficients = model.coef_[0]\n",
        "\n",
        "# Get the feature names\n",
        "features = X.columns\n",
        "\n",
        "# Identify important features\n",
        "important_features = sorted(zip(features, coefficients), key=lambda x: abs(x[1]), reverse=True)\n",
        "\n",
        "print(\"Important features based on model coefficients:\")\n",
        "for feature, coef in important_features:\n",
        "    print(f\"{feature}: {coef}\")"
      ],
      "metadata": {
        "id": "CzQQbzqdh0on"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###19. Write a Python program to train Logistic Regression and evaluate its performance using Cohen‚Äôs Kappa Score."
      ],
      "metadata": {
        "id": "rfUAXdwQh5k-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import cohen_kappa_score\n",
        "\n",
        "# Train Logistic Regression model\n",
        "model = LogisticRegression(max_iter=1000)\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Predict\n",
        "y_pred = model.predict(X_test)\n",
        "\n",
        "# Cohen's Kappa score\n",
        "kappa_score = cohen_kappa_score(y_test, y_pred)\n",
        "print(\"Cohen's Kappa Score:\", kappa_score)"
      ],
      "metadata": {
        "id": "TIsTAOQAh-e-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###20. Write a Python program to train Logistic Regression and visualize the Precision-Recall Curve for binary classificatio."
      ],
      "metadata": {
        "id": "zGD4MUW8iEB2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import precision_recall_curve\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Train Logistic Regression model\n",
        "model = LogisticRegression(max_iter=1000)\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Predict probabilities\n",
        "y_prob = model.predict_proba(X_test)[:, 1]\n",
        "\n",
        "# Precision-Recall curve\n",
        "precision, recall, _ = precision_recall_curve(y_test, y_prob)\n",
        "\n",
        "# Plot the Precision-Recall curve\n",
        "plt.plot(recall, precision, marker='.', color='b')\n",
        "plt.title('Precision-Recall Curve')\n",
        "plt.xlabel('Recall')\n",
        "plt.ylabel('Precision')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "le1yNeXCiKNG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###21. Write a Python program to train Logistic Regression with different solvers (liblinear, saga, lbfgs) and compare their accuracy."
      ],
      "metadata": {
        "id": "QzVVjRodiOzP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "solvers = ['liblinear', 'saga', 'lbfgs']\n",
        "accuracy_dict = {}\n",
        "\n",
        "# Train Logistic Regression with different solvers and compare accuracy\n",
        "for solver in solvers:\n",
        "    model = LogisticRegression(solver=solver, max_iter=1000)\n",
        "    model.fit(X_train, y_train)\n",
        "    y_pred = model.predict(X_test)\n",
        "    accuracy = accuracy_score(y_test, y_pred)\n",
        "    accuracy_dict[solver] = accuracy\n",
        "\n",
        "# Print comparison\n",
        "for solver, accuracy in accuracy_dict.items():\n",
        "    print(f\"Accuracy with solver '{solver}':\", accuracy)"
      ],
      "metadata": {
        "id": "w434A93_iUpm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###22. Write a Python program to train Logistic Regression and evaluate its performance using Matthews Correlation Coefficient (MCC)."
      ],
      "metadata": {
        "id": "vUhvrLTBiZjd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import matthews_corrcoef\n",
        "\n",
        "# Train Logistic Regression model\n",
        "model = LogisticRegression(max_iter=1000)\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Predict\n",
        "y_pred = model.predict(X_test)\n",
        "\n",
        "# Matthews Correlation Coefficient (MCC)\n",
        "mcc_score = matthews_corrcoef(y_test, y_pred)\n",
        "print(\"Matthews Correlation Coefficient (MCC):\", mcc_score)"
      ],
      "metadata": {
        "id": "yDHYRaZiiew1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###23. Write a Python program to train Logistic Regression on both raw and standardized data. Compare their accuracy to see the impact of feature scaling."
      ],
      "metadata": {
        "id": "QqcyICiYijnV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Train Logistic Regression model on raw data\n",
        "model_raw = LogisticRegression(max_iter=1000)\n",
        "model_raw.fit(X_train, y_train)\n",
        "y_pred_raw = model_raw.predict(X_test)\n",
        "accuracy_raw = accuracy_score(y_test, y_pred_raw)\n",
        "\n",
        "# Standardize features\n",
        "scaler = StandardScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "X_test_scaled = scaler.transform(X_test)\n",
        "\n",
        "# Train Logistic Regression model on standardized data\n",
        "model_scaled = LogisticRegression(max_iter=1000)\n",
        "model_scaled.fit(X_train_scaled, y_train)\n",
        "y_pred_scaled = model_scaled.predict(X_test_scaled)\n",
        "accuracy_scaled = accuracy_score(y_test, y_pred_scaled)\n",
        "\n",
        "# Compare accuracy\n",
        "print(\"Accuracy on raw data:\", accuracy_raw)\n",
        "print(\"Accuracy on standardized data:\", accuracy_scaled)"
      ],
      "metadata": {
        "id": "i_ORjQ8EipEN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###24. Write a Python program to train Logistic Regression and find the optimal C (regularization strength) using cross-validation."
      ],
      "metadata": {
        "id": "GoixzEqfit7V"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import cross_val_score\n",
        "\n",
        "# Train Logistic Regression model with cross-validation\n",
        "model = LogisticRegression(max_iter=1000)\n",
        "\n",
        "# Evaluate using cross-validation to find the optimal C\n",
        "C_values = [0.01, 0.1, 1, 10, 100]\n",
        "best_C = None\n",
        "best_score = 0\n",
        "\n",
        "for C in C_values:\n",
        "    model.C = C\n",
        "    cv_scores = cross_val_score(model, X_train, y_train, cv=5, scoring='accuracy')\n",
        "    mean_score = cv_scores.mean()\n",
        "    if mean_score > best_score:\n",
        "        best_C = C\n",
        "        best_score = mean_score\n",
        "\n",
        "print(\"Optimal C value:\", best_C)\n",
        "print(\"Best cross-validation accuracy:\", best_score)"
      ],
      "metadata": {
        "id": "ZjjSTjZOi0h0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###25. Write a Python program to train Logistic Regression, save the trained model using joblib, and load it again to make predictions."
      ],
      "metadata": {
        "id": "Bv0Of0-pi5Fs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import joblib\n",
        "\n",
        "# Train Logistic Regression model\n",
        "model = LogisticRegression(max_iter=1000)\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Save the model using joblib\n",
        "joblib.dump(model, 'logistic_regression_model.pkl')\n",
        "\n",
        "# Load the model\n",
        "loaded_model = joblib.load('logistic_regression_model.pkl')\n",
        "\n",
        "# Make predictions with the loaded model\n",
        "y_pred = loaded_model.predict(X_test)\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "\n",
        "print(\"Accuracy with loaded model:\", accuracy)"
      ],
      "metadata": {
        "id": "g20D8eBhi__s"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n"
      ],
      "metadata": {
        "id": "2_SsS6I5KPFL"
      }
    }
  ]
}